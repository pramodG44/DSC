{"nbformat_minor": 1, "cells": [{"source": "### SPAM CLASSIFICATION", "cell_type": "markdown", "metadata": {}}, {"source": "!wget spamham.txt https://raw.githubusercontent.com/vincechan/spam-detection/master/data/SpamDetectionData.txt", "cell_type": "code", "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2019-08-25 05:24:11--  http://spamham.txt/\nResolving spamham.txt (spamham.txt)... failed: Name or service not known.\nwget: unable to resolve host address \u2018spamham.txt\u2019\n--2019-08-25 05:24:11--  https://raw.githubusercontent.com/vincechan/spam-detection/master/data/SpamDetectionData.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3602319 (3.4M) [text/plain]\nSaving to: \u2018SpamDetectionData.txt\u2019\n\n100%[======================================>] 3,602,319   --.-K/s   in 0.1s    \n\n2019-08-25 05:24:12 (35.1 MB/s) - \u2018SpamDetectionData.txt\u2019 saved [3602319/3602319]\n\nFINISHED --2019-08-25 05:24:12--\nTotal wall clock time: 0.5s\nDownloaded: 1 files, 3.4M in 0.1s (35.1 MB/s)\n"}], "metadata": {}}, {"source": "import random\n\ndef read_file(path):\n    \"\"\"\n    read and return all data in a file\n    \"\"\"\n    with open(path, 'r') as f:\n        return f.read()\n\ndef load_data():\n    \"\"\"\n    load and return the data in features and labels lists\n    each item in features contains the raw email text\n    each item in labels is either 1(spam) or 0(ham) and identifies corresponding item in features\n    \"\"\"\n    # load all data from file\n    data_path = \"SpamDetectionData.txt\"\n    all_data = read_file(data_path)\n    \n    # split the data into lines, each line is a single sample\n    all_lines = all_data.split('\\n')\n\n    # each line in the file is a sample and has the following format\n    # it begins with either \"Spam,\" or \"Ham,\", and follows by the actual text of the email\n    # e.g. Spam,<p>His honeyed and land....\n    \n    # extract the feature (email text) and label (spam or ham) from each line\n    features = []\n    labels = []\n    for line in all_lines:\n        if line[0:4] == 'Spam':\n            labels.append(1)\n            features.append(line[5:])\n            pass\n        elif line[0:3] == 'Ham':\n            labels.append(0)\n            features.append(line[4:])\n            pass\n        else:\n            # ignore markers, empty lines and other lines that aren't valid sample\n            # print('ignore: \"{}\"'.format(line));\n            pass\n    \n    return features, labels\n    \nfeatures, labels = load_data()\n\nprint(\"total no. of samples: {}\".format(len(labels)))\nprint(\"total no. of spam samples: {}\".format(labels.count(1)))\nprint(\"total no. of ham samples: {}\".format(labels.count(0)))\n\nprint(\"\\nPrint a random sample for inspection:\")\nrandom_idx = random.randint(0, len(labels))\nprint(\"example feature: {}\".format(features[random_idx][0:]))\nprint(\"example label: {} ({})\".format(labels[random_idx], 'spam' if labels[random_idx] else 'ham'))", "cell_type": "code", "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": "total no. of samples: 2100\ntotal no. of spam samples: 1043\ntotal no. of ham samples: 1057\n\nPrint a random sample for inspection:\nexample feature: <p>Harold prose and from he maddest deem finds will were mine and he from. To spoiled for pile loved deem power he dwelt sadness longed one made long these sister that and oft. One was carnal below in losel. Worse spent for name within now it and deadly and to though have she. He vaunted whence native heralds left in there into in. Parasites reverie day evil dear now favour shamed and come the feud.</p><p>Concubines flaunting uncouth had smile mirthful yes soul a few days the but few carnal had revellers. To partings were and known be paphian woe not he. Condole sighed little glee he weary florid will forgot now seek had all scene time sore deemed that low.</p>\nexample label: 1 (spam)\n"}], "metadata": {}}, {"source": "from sklearn.model_selection import train_test_split\n\n# load features and labels\nfeatures, labels = load_data()\n\n# split data into training / test sets\nfeatures_train, features_test, labels_train, labels_test = train_test_split(\n    features, \n    labels, \n    test_size=0.2,   # use 10% for testing\n    random_state=42)\n\nprint(\"no. of train features: {}\".format(len(features_train)))\nprint(\"no. of train labels: {}\".format(len(labels_train)))\nprint(\"no. of test features: {}\".format(len(features_test)))\nprint(\"no. of test labels: {}\".format(len(labels_test)))", "cell_type": "code", "execution_count": 4, "outputs": [{"output_type": "stream", "name": "stdout", "text": "no. of train features: 1680\nno. of train labels: 1680\nno. of test features: 420\nno. of test labels: 420\n"}], "metadata": {}}, {"source": "## Bag of words", "cell_type": "markdown", "metadata": {}}, {"source": "documents = ['Hello, how are you!',\n             'Win money, win from home.',\n             'Call me now.',\n             'Hello, Call hello you tomorrow?']\n\nlower_case_documents = []\nlower_case_documents = [d.lower() for d in documents]\nprint(lower_case_documents)", "cell_type": "code", "execution_count": 5, "outputs": [{"output_type": "stream", "name": "stdout", "text": "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"}], "metadata": {}}, {"source": "sans_punctuation_documents = []\nimport string\n\nfor i in lower_case_documents:\n    sans_punctuation_documents.append(i.translate(str.maketrans(\"\",\"\", string.punctuation)))\n    \nsans_punctuation_documents", "cell_type": "code", "execution_count": 6, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "['hello how are you',\n 'win money win from home',\n 'call me now',\n 'hello call hello you tomorrow']"}, "execution_count": 6, "metadata": {}}], "metadata": {}}, {"source": "preprocessed_documents = [[w for w in d.split()] for d in sans_punctuation_documents]\npreprocessed_documents", "cell_type": "code", "execution_count": 7, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[['hello', 'how', 'are', 'you'],\n ['win', 'money', 'win', 'from', 'home'],\n ['call', 'me', 'now'],\n ['hello', 'call', 'hello', 'you', 'tomorrow']]"}, "execution_count": 7, "metadata": {}}], "metadata": {}}, {"source": "frequency_list = []\nimport pprint\nfrom collections import Counter\n\nfrequency_list = [Counter(d) for d in preprocessed_documents]\npprint.pprint(frequency_list)", "cell_type": "code", "execution_count": 8, "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n Counter({'call': 1, 'me': 1, 'now': 1}),\n Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"}], "metadata": {}}, {"source": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()", "cell_type": "code", "execution_count": 9, "outputs": [], "metadata": {}}, {"source": "count_vector.fit(documents)\ncount_vector.get_feature_names()", "cell_type": "code", "execution_count": 10, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "['are',\n 'call',\n 'from',\n 'hello',\n 'home',\n 'how',\n 'me',\n 'money',\n 'now',\n 'tomorrow',\n 'win',\n 'you']"}, "execution_count": 10, "metadata": {}}], "metadata": {}}, {"source": "doc_array = count_vector.transform(documents).toarray()\ndoc_array", "cell_type": "code", "execution_count": 11, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]])"}, "execution_count": 11, "metadata": {}}], "metadata": {}}, {"source": "import pandas as pd\nfrequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names())\nfrequency_matrix", "cell_type": "code", "execution_count": 13, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n0    1     0     0      1     0    1   0      0    0         0    0    1\n1    0     0     1      0     1    0   0      1    0         0    2    0\n2    0     1     0      0     0    0   1      0    1         0    0    0\n3    0     1     0      2     0    0   0      0    0         1    0    1", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>are</th>\n      <th>call</th>\n      <th>from</th>\n      <th>hello</th>\n      <th>home</th>\n      <th>how</th>\n      <th>me</th>\n      <th>money</th>\n      <th>now</th>\n      <th>tomorrow</th>\n      <th>win</th>\n      <th>you</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 13, "metadata": {}}], "metadata": {}}, {"source": "## TFIDF", "cell_type": "markdown", "metadata": {}}, {"source": "from sklearn.feature_extraction.text import TfidfVectorizer\n# this is a very toy example, do not try this at home unless you want to understand the usage differences\ndocs=[\"the house had a tiny little mouse\",\n      \"the cat saw the mouse\",\n      \"the mouse ran away from the house\",\n      \"the cat finally ate the mouse\",\n      \"the end of the mouse story\"\n     ]\n \n# settings that you use for count vectorizer will go here\ntfidf_vectorizer=TfidfVectorizer(use_idf=True)\n \n# just send in all your docs here\ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n# get the first vector out (for the first document)\nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n \n# place tf-idf values in a pandas data frame\ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\ndf.sort_values(by=[\"tfidf\"],ascending=False)", "cell_type": "code", "execution_count": 28, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "            tfidf\nhad      0.493562\nlittle   0.493562\ntiny     0.493562\nhouse    0.398203\nmouse    0.235185\nthe      0.235185\nate      0.000000\naway     0.000000\ncat      0.000000\nend      0.000000\nfinally  0.000000\nfrom     0.000000\nof       0.000000\nran      0.000000\nsaw      0.000000\nstory    0.000000", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>had</th>\n      <td>0.493562</td>\n    </tr>\n    <tr>\n      <th>little</th>\n      <td>0.493562</td>\n    </tr>\n    <tr>\n      <th>tiny</th>\n      <td>0.493562</td>\n    </tr>\n    <tr>\n      <th>house</th>\n      <td>0.398203</td>\n    </tr>\n    <tr>\n      <th>mouse</th>\n      <td>0.235185</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0.235185</td>\n    </tr>\n    <tr>\n      <th>ate</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>away</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>cat</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>end</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>finally</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>from</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ran</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>saw</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>story</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 28, "metadata": {}}], "metadata": {}}, {"source": "from sklearn.feature_extraction.text import TfidfVectorizer\n# this is a very toy example, do not try this at home unless you want to understand the usage differences\ndocs=[\"hotel food service\",\n    \"hote food service\"\n     ]\n \n# settings that you use for count vectorizer will go here\ntfidf_vectorizer=TfidfVectorizer(use_idf=True)\n \n# just send in all your docs here\ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n# get the first vector out (for the first document)\nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[1]\n \n# place tf-idf values in a pandas data frame\ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\ndf.sort_values(by=[\"tfidf\"],ascending=False)", "cell_type": "code", "execution_count": 31, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "            tfidf\nhote     0.704909\nfood     0.501549\nservice  0.501549\nhotel    0.000000", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>hote</th>\n      <td>0.704909</td>\n    </tr>\n    <tr>\n      <th>food</th>\n      <td>0.501549</td>\n    </tr>\n    <tr>\n      <th>service</th>\n      <td>0.501549</td>\n    </tr>\n    <tr>\n      <th>hotel</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 31, "metadata": {}}], "metadata": {}}, {"source": "from sklearn.feature_extraction.text import TfidfVectorizer\n\n# vectorize email text into tfidf matrix\n# TfidfVectorizer converts collection of raw documents to a matrix of TF-IDF features.\n# It's equivalent to CountVectorizer followed by TfidfTransformer.\nvectorizer = TfidfVectorizer(\n    input='content',     # input is actual text\n    lowercase=True,      # convert to lower case before tokenizing\n    stop_words='english' # remove stop words\n)\nfeatures_train_transformed = vectorizer.fit_transform(features_train)\nfeatures_test_transformed  = vectorizer.transform(features_test)", "cell_type": "code", "execution_count": 32, "outputs": [], "metadata": {}}, {"source": "from sklearn.naive_bayes import MultinomialNB\nimport pickle\n\ndef save(vectorizer, classifier):\n    '''\n    save classifier to disk\n    '''\n    with open('model.pkl', 'wb') as file:\n        pickle.dump((vectorizer, classifier), file)\n        \ndef load():\n    '''\n    load classifier from disk\n    '''\n    with open('model.pkl', 'rb') as file:\n      vectorizer, classifier = pickle.load(file)\n    return vectorizer, classifier\n\n# train a classifier\nclassifier = MultinomialNB()\nclassifier.fit(features_train_transformed, labels_train)\n\n# save the trained model\nsave(vectorizer, classifier)\n\n# score the classifier accuracy\nprint(\"classifier accuracy {:.2f}%\".format(classifier.score(features_test_transformed, labels_test) * 100))", "cell_type": "code", "execution_count": 37, "outputs": [{"output_type": "stream", "name": "stdout", "text": "classifier accuracy 100.00%\n"}], "metadata": {}}, {"source": "import numpy as np\nfrom sklearn import metrics\nprediction = classifier.predict(features_test_transformed)\nfscore = metrics.f1_score(labels_test, prediction, average='macro')\nprint(\"F score {:.2f}\".format(fscore))", "cell_type": "code", "execution_count": 38, "outputs": [{"output_type": "stream", "name": "stdout", "text": "F score 1.00\n"}], "metadata": {}}, {"source": "vectorizer, classifer = load()\n\nprint('\\nPerform a test')                    \n#email_input = 'enter your email here'\nemail_input = [\"\"\"<p>Register below and join Rajesh Garg (Chief Financial Officer, Landmark Group) and Shail Khiyara (Customer Experience Officer, UiPath) to learn some amazing insights on Landmark Group\u2019s journey into digital transformation through automation, how they addressed the retail industry\u2019s complexity with RPA and find out the top-of-mind CFO challenges that automation can solve quickly.  </p>\"\"\"]\nemail_input_transformed = vectorizer.transform(email_input)\nprediction = classifer.predict(email_input_transformed)\n\nprint('EMAIL:', email_input)\nprint('The email is', 'SPAM' if prediction else 'HAM')", "cell_type": "code", "execution_count": 41, "outputs": [{"output_type": "stream", "name": "stdout", "text": "\nPerform a test\nEMAIL: ['<p>Register below and join Rajesh Garg (Chief Financial Officer, Landmark Group) and Shail Khiyara (Customer Experience Officer, UiPath) to learn some amazing insights on Landmark Group\u2019s journey into digital transformation through automation, how they addressed the retail industry\u2019s complexity with RPA and find out the top-of-mind CFO challenges that automation can solve quickly.  </p>']\nThe email is HAM\n"}], "metadata": {}}, {"source": "----------------------------------------\n### Sentiment Analysis", "cell_type": "markdown", "metadata": {}}, {"source": "nltk.download('movie_reviews')", "cell_type": "code", "execution_count": 43, "outputs": [{"output_type": "stream", "name": "stderr", "text": "[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"}, {"output_type": "execute_result", "data": {"text/plain": "True"}, "execution_count": 43, "metadata": {}}], "metadata": {}}, {"source": "# Load and prepare the dataset\nimport nltk\nfrom nltk.corpus import movie_reviews\nimport random\n\ndocuments = [(list(movie_reviews.words(fileid)), category)\n              for category in movie_reviews.categories()\n              for fileid in movie_reviews.fileids(category)]\n\nrandom.shuffle(documents)", "cell_type": "code", "execution_count": 44, "outputs": [], "metadata": {}}, {"source": "# Define the feature extractor\n\nall_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\nword_features = list(all_words)[:2000]\n\ndef document_features(document):\n    document_words = set(document)\n    features = {}\n    for word in word_features:\n        features['contains({})'.format(word)] = (word in document_words)\n    return features", "cell_type": "code", "execution_count": 45, "outputs": [], "metadata": {}}, {"source": "# Train Naive Bayes classifier\nfeaturesets = [(document_features(d), c) for (d,c) in documents]\ntrain_set, test_set = featuresets[100:], featuresets[:100]\nclassifier = nltk.NaiveBayesClassifier.train(train_set)", "cell_type": "code", "execution_count": 46, "outputs": [], "metadata": {}}, {"source": "# Test the classifier\nprint(nltk.classify.accuracy(classifier, test_set))", "cell_type": "code", "execution_count": 47, "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.74\n"}], "metadata": {"scrolled": true}}, {"source": "classifier.show_most_informative_features(20)", "cell_type": "code", "execution_count": 50, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Most Informative Features\n    contains(schumacher) = True              neg : pos    =     11.8 : 1.0\n        contains(turkey) = True              neg : pos    =      8.2 : 1.0\n       contains(stellan) = True              pos : neg    =      7.6 : 1.0\n contains(unimaginative) = True              neg : pos    =      7.1 : 1.0\n        contains(shoddy) = True              neg : pos    =      7.1 : 1.0\n        contains(temper) = True              pos : neg    =      6.9 : 1.0\n     contains(atrocious) = True              neg : pos    =      6.7 : 1.0\n         contains(kudos) = True              pos : neg    =      6.5 : 1.0\n          contains(mena) = True              neg : pos    =      6.4 : 1.0\n        contains(suvari) = True              neg : pos    =      6.4 : 1.0\n       contains(singers) = True              pos : neg    =      6.3 : 1.0\n        contains(poorly) = True              neg : pos    =      6.0 : 1.0\n      contains(explores) = True              pos : neg    =      5.8 : 1.0\n        contains(wasted) = True              neg : pos    =      5.6 : 1.0\n       contains(unravel) = True              pos : neg    =      5.6 : 1.0\n           contains(ugh) = True              neg : pos    =      5.5 : 1.0\n         contains(awful) = True              neg : pos    =      5.2 : 1.0\n        contains(justin) = True              neg : pos    =      5.1 : 1.0\n        contains(welles) = True              neg : pos    =      5.1 : 1.0\n     contains(underwood) = True              neg : pos    =      5.1 : 1.0\n"}], "metadata": {}}, {"source": "###  Lemmatization", "cell_type": "markdown", "metadata": {}}, {"source": "nltk.download('wordnet')", "cell_type": "code", "execution_count": 52, "outputs": [{"output_type": "stream", "name": "stderr", "text": "[nltk_data] Downloading package wordnet to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n"}, {"output_type": "execute_result", "data": {"text/plain": "True"}, "execution_count": 52, "metadata": {}}], "metadata": {}}, {"source": "# import these modules \nfrom nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer() \n  \nprint(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \nprint(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \nprint(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) ", "cell_type": "code", "execution_count": 57, "outputs": [{"output_type": "stream", "name": "stdout", "text": "rocks : rock\ncorpora : corpus\nbetter : better\n"}], "metadata": {}}, {"source": "### Stemming", "cell_type": "markdown", "metadata": {}}, {"source": "# import these modules \nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize \n   \nps = PorterStemmer() \n  \n# choose some words to be stemmed \nwords = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n  \nfor w in words: \n    print(w, \" : \", ps.stem(w)) ", "cell_type": "code", "execution_count": 58, "outputs": [{"output_type": "stream", "name": "stdout", "text": "program  :  program\nprograms  :  program\nprogramer  :  program\nprograming  :  program\nprogramers  :  program\n"}], "metadata": {}}, {"source": "nltk.download('punkt')", "cell_type": "code", "execution_count": 60, "outputs": [{"output_type": "stream", "name": "stderr", "text": "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"}, {"output_type": "execute_result", "data": {"text/plain": "True"}, "execution_count": 60, "metadata": {}}], "metadata": {}}, {"source": "# importing modules \nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize \n   \nps = PorterStemmer() \n   \nsentence = \"Programers program with programing languages\"\nwords = word_tokenize(sentence) \n   \nfor w in words: \n    print(w, \" : \", ps.stem(w)) ", "cell_type": "code", "execution_count": 61, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Programers  :  program\nprogram  :  program\nwith  :  with\nprograming  :  program\nlanguages  :  languag\n"}], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3.6", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.8", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}}